{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaledBadran/FairBoost/blob/initial_design/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CymiO6Y8k-2R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import random\n",
        "\n",
        "# from FairBoost import FairBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_PlC4VQk-2W"
      },
      "outputs": [],
      "source": [
        "def function1(instance):\n",
        "    return instance\n",
        "\n",
        "def function2(instance):\n",
        "    return instance * [[random.random()] for _ in range(len(instance))]\n",
        "\n",
        "preprocessing1= lambda data:function1(data)\n",
        "preprocessing2= lambda data:function2(data)\n",
        "preprocessing = (preprocessing1, preprocessing2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j96PbQXEk-2X",
        "outputId": "c0ee2223-01b4-4180-a153-281a39d893e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7017543859649122 0.9777777777777777 0.5714285714285714\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_breast_cancer()\n",
        "X    = data.data\n",
        "y    = data.target\n",
        "model = DecisionTreeClassifier(class_weight='balanced')\n",
        "\n",
        "## declare an ensemble instance with default parameters ##\n",
        "\n",
        "data = {'X': X, 'y': y}\n",
        "ens = FairBoost(data, model, preprocessing)\n",
        "\n",
        "## train the ensemble & view estimates for prediction error ##\n",
        "ens.train_models()\n",
        "ens.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "XvzEb75Vk-2Z"
      },
      "outputs": [],
      "source": [
        "## imports ##\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import scipy.spatial.distance as dist\n",
        "\n",
        "\n",
        "from enum import Enum\n",
        "class Bootstrap_type(Enum):\n",
        "    NONE=1\n",
        "    DEFAULT=2\n",
        "    CUSTOM=3\n",
        "\n",
        "## Note: FairBoost tries to replicate sklearn API\n",
        "class FairBoost(object):\n",
        "    def __init__(self, model, preprocessing_functions, bootstrap_type=Bootstrap_type.DEFAULT, bootstrap_size=0.63):\n",
        "        self.model = model\n",
        "        self.preprocessing_functions = preprocessing_functions\n",
        "        self.n_elements = len(preprocessing_functions)\n",
        "        self.bootstrap_size = bootstrap_size\n",
        "        self.bootstrap_type = bootstrap_type\n",
        "\n",
        "        # The trained models\n",
        "        self.models = []\n",
        "        # TODO: consider other distance functions\n",
        "        self.dist_func = dist.cosine\n",
        "        # ipdb.set_trace(context=6)\n",
        "        \n",
        "\n",
        "    # Generates all \"cleaned\" data sets\n",
        "    # Returns an array of (X,y)\n",
        "    def __preprocess_data(self, X, y):\n",
        "        pp_data = []\n",
        "        for ppf in self.preprocessing_functions:\n",
        "            pp_data.append(ppf(X, y))\n",
        "        return pp_data\n",
        "\n",
        "    def __get_avg_dist_arr(self, data):\n",
        "        # Swap the first two dimensions so we iterate over instances instead of data sets\n",
        "        data = data.transpose([1, 0, 2])\n",
        "        # Initializing the average distances array\n",
        "        dist_arr = np.zeros(\n",
        "            shape=(len(data), len(self.preprocessing_functions)))\n",
        "        # Fill the avg distances array\n",
        "        for i, pp_instances in enumerate(data):\n",
        "            for j, pp_instance_j in enumerate(pp_instances):\n",
        "                distances = []\n",
        "                for k, pp_instance_k in enumerate(pp_instances):\n",
        "                    d = self.dist_func( pp_instance_j, pp_instance_k)\n",
        "                    d = np.abs(d)\n",
        "                    distances.append(d)\n",
        "                dist_arr[i, j] = np.mean(distances)\n",
        "        dist_arr = dist_arr.transpose([1, 0])\n",
        "        # Normalize\n",
        "        n_dist_arr = []\n",
        "        for arr in dist_arr:\n",
        "            s = np.sum(arr)\n",
        "            n = arr/s\n",
        "            n_dist_arr.append(n)\n",
        "        return n_dist_arr\n",
        "\n",
        "    # Adds y to the last column of X for a list of (X,y)\n",
        "    def __merge_Xy(self, datasets):\n",
        "        res = []\n",
        "        for dataset in datasets:\n",
        "            X, y = dataset[0], np.expand_dims(dataset[1], axis=-1)\n",
        "            m = np.concatenate([X, y], axis=-1)\n",
        "            res.append(m)\n",
        "        return np.array(res)\n",
        "\n",
        "    # Generate the boostrap data sets\n",
        "    # Returns a list of (X,y)\n",
        "    def __bootstrap_datasets(self, X, y):\n",
        "        datasets = self.__preprocess_data(X, y)\n",
        "        datasets = self.__merge_Xy(datasets)\n",
        "        # If we do the custom bootstrapping, we must define a custom PDF\n",
        "        if self.bootstrap_type == Bootstrap_type.CUSTOM:\n",
        "            dist_arrays = self.__get_avg_dist_arr(datasets)\n",
        "        else:\n",
        "            dist_arrays = [None for _ in range(len(datasets))]\n",
        "\n",
        "        bootstrap_datasets = []\n",
        "        for dataset, dist_arr in zip(datasets, dist_arrays):\n",
        "            indexes = [i for i in range(len(dataset))]\n",
        "            size = int(self.bootstrap_size*len(dataset))\n",
        "            indexes = np.random.choice(indexes, size=size, replace=True, p=dist_arr)\n",
        "            bootstrap_datasets.append((dataset[indexes,:-1], dataset[indexes,-1]))\n",
        "\n",
        "        return bootstrap_datasets\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        datasets = self.__bootstrap_datasets(X,y)\n",
        "        for X_bootstrap, y_bootstrap in datasets:\n",
        "            model = clone(self.model)\n",
        "            model.fit(X_bootstrap, y_bootstrap)\n",
        "            self.models.append(model)\n",
        "        return self\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        accs = np.array([])\n",
        "        pres = np.array([])\n",
        "        recs = np.array([])\n",
        "        for i in range(len(self.models)):\n",
        "            yp = self.models[i].predict(self.preprocessed_data[i]['test'][0])\n",
        "            acc = accuracy_score(self.preprocessed_data[i]['test'][1], yp)\n",
        "            pre = precision_score(self.preprocessed_data[i]['test'][1], yp)\n",
        "            rec = recall_score(self.preprocessed_data[i]['test'][1], yp)\n",
        "            # store the error metrics\n",
        "            accs = np.concatenate((accs, acc.flatten()))\n",
        "            pres = np.concatenate((pres, pre.flatten()))\n",
        "            recs = np.concatenate((recs, rec.flatten()))\n",
        "        print(accs, pres, recs)\n",
        "\n",
        "    # def predict(self):\n",
        "    #     predictions = []\n",
        "    #     for i in range(len(self.models)):\n",
        "    #         yp = self.models[i].predict(self.preprocessed_data[i]['test'][0])\n",
        "    #         predictions.append(yp.reshape(-1, 1))\n",
        "    #     ypred = np.round(np.mean(np.concatenate(\n",
        "    #         predictions, axis=1), axis=1)).astype(int)\n",
        "    #     acc = accuracy_score(self.preprocessed_data[0]['test'][1], ypred)\n",
        "    #     pre = precision_score(self.preprocessed_data[0]['test'][1], ypred)\n",
        "    #     rec = recall_score(self.preprocessed_data[0]['test'][1], ypred)\n",
        "    #     print(acc, pre, rec)\n",
        "    #     return (ypred)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for i in range(len(self.models)):\n",
        "            y_pred.append(self.models[i].predict(X))\n",
        "        # Computing a soft majority voting\n",
        "        y_pred = np.array(y_pred).transpose()\n",
        "        y_pred = np.mean(y_pred, axis=-1).astype(int)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "vobIi28Rk-2c",
        "outputId": "81276933-5a09-4fb1-e1d7-5e7a7e951393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting toml>=0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=18f1bbb16e15ae28bbc9c8f9533412623e58ebb336aa9a0fd8e98f731625ec20\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, toml, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.32.0 prompt-toolkit-3.0.28 toml-0.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install ipdb\n",
        "import  ipdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz5SYmWMlLCu",
        "outputId": "22e31342-9080-4231-8faa-40786143316f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZlSSKBKnk-2d"
      },
      "outputs": [],
      "source": [
        "def function1(instance, label):\n",
        "    return instance, label\n",
        "\n",
        "def function2(instance, label):\n",
        "    return instance * [[random.random()] for _ in range(len(instance))], label\n",
        "\n",
        "preprocessing = (function1, function2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IaI63CNk-2e",
        "outputId": "278ee334-5621-4ed9-81b7-9237f2500418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9468085106382979"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X    = data.data\n",
        "y    = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "model = LogisticRegression()\n",
        "\n",
        "ens = FairBoost(model, preprocessing)\n",
        "ens = ens.fit(X_train,y_train)\n",
        "y_pred = ens.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline\n",
        "model = DecisionTreeClassifier(class_weight='balanced')\n",
        "model = model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsNFtLUCyqD9",
        "outputId": "bbeb2fb3-0713-4c25-f92d-d1f8ad28a86b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9042553191489362"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "byaeO9eNyuOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}