{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# import plotnine as pn\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_baseline_new(path):\n",
    "    \"\"\"\n",
    "    Generates a dict object representing the data in the baseline_splits.json.\n",
    "            Parameters:\n",
    "                    path : path of the file baseline_splits.json\n",
    "            Returns:\n",
    "                    dict (Dict): a preprocessed dict representing the data\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    for dataset_key, dataset_value in results[\"results\"].items():\n",
    "        for preprocessing_key, preprocessing_value in dataset_value.items():\n",
    "            if preprocessing_key == 'baseline':\n",
    "                for classifier_key, classifier_value in preprocessing_value.items():\n",
    "                    key = dataset_key + \"-\" + preprocessing_key + \"-\" + classifier_key\n",
    "                    dict[key] = {'dataset':dataset_key, 'preprocessing':preprocessing_key, 'classifier':classifier_key, 'classifier_value':classifier_value}\n",
    "            elif preprocessing_key != \"DisparateImpactRemover\":\n",
    "                for classifier_key, classifier_value in preprocessing_value[0][\"results\"].items():\n",
    "                    key = dataset_key + \"-\" + preprocessing_key + \"-\" + classifier_key\n",
    "                    dict[key] = {'dataset':dataset_key, 'preprocessing':preprocessing_key, 'classifier':classifier_key, 'classifier_value':classifier_value}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_fairboost_new(path):\n",
    "    \"\"\"\n",
    "    Generates a dict object representing the data in the fairboost_splits.json.\n",
    "            Parameters:\n",
    "                    path : path of the file fairboost_splits.json\n",
    "            Returns:\n",
    "                    dict (Dict): a preprocessed dict representing the data\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for dataset_key, dataset_value in results[\"results\"].items():\n",
    "        for preprocessing_key, preprocessing_value in dataset_value.items():\n",
    "            if preprocessing_key == \"fairboost\":\n",
    "                for i in range(6, len(preprocessing_value), 7):\n",
    "                    for classifier_key, classifier_value in preprocessing_value[i][\"results\"].items():\n",
    "                        key = \"Fairboost : \" + dataset_key + \"-\" + preprocessing_key + \"-\" + classifier_key + \"-\" \\\n",
    "                              + preprocessing_value[i][\"hyperparameters\"][\"init\"]['bootstrap_type']\n",
    "                        dict[key] = {'dataset':dataset_key, 'preprocessing':preprocessing_value[i][\"hyperparameters\"][\"init\"]['bootstrap_type'], 'classifier':classifier_key, 'classifier_value':classifier_value}\n",
    "#             else:\n",
    "#                 for i in range(len(preprocessing_value)):\n",
    "#                     for classifier_key, classifier_value in preprocessing_value[i][\"results\"].items():\n",
    "#                         if preprocessing_value[i][\"hyperparameters\"]['bootstrap_type'] == \"NONE\":\n",
    "#                             key = \"Fairboost : \" + dataset_key + \"-\" + preprocessing_key + \"-\" + classifier_key + \"-\" \\\n",
    "#                                   + preprocessing_value[i][\"hyperparameters\"]['bootstrap_type']\n",
    "#                             dict[key] = {'dataset':dataset_key, 'preprocessing':preprocessing_value[i][\"hyperparameters\"]['bootstrap_type'], 'classifier':classifier_key, 'classifier_value':classifier_value}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def to_dataframe(data: Dict, dataset_name=\"\", classifier_name=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame object, necessary for the plotting.\n",
    "            Parameters:\n",
    "                    data : List of preprocessing dicts.\n",
    "            Returns:\n",
    "                    d (DataFrame): returns the dataframe\n",
    "    \"\"\"\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    t = []\n",
    "    for key, value in data.items():\n",
    "        if (dataset_name in key) and (classifier_name in key):\n",
    "            mean_accuracy = np.mean(value[\"accuracy\"])\n",
    "            mean_fairness = np.mean(value[\"disparate_impact\"])\n",
    "            std_accuracy = np.std(value[\"accuracy\"])\n",
    "            std_fairness = np.std(value[\"disparate_impact\"])\n",
    "            x1.append(mean_accuracy - (std_accuracy / 2))\n",
    "            x2.append(mean_accuracy + (std_accuracy / 2))\n",
    "            y1.append(mean_fairness - (std_fairness / 2))\n",
    "            y2.append(mean_fairness + (std_fairness / 2))\n",
    "            t.append(key)\n",
    "    d = pd.DataFrame({\"x1\": x1, \"x2\": x2, \"y1\": y1, \"y2\": y2, \"t\": t, \"r\": t})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def read_data() -> Dict:\n",
    "    \"\"\"\n",
    "    Read data from files and return its content in dictionnaries.\n",
    "            Returns:\n",
    "                    data: the data contained in both files\n",
    "    \"\"\"\n",
    "    data_path = Path(\"raw_data\")\n",
    "    fairboost_results_path = Path(data_path, 'fairboost_splits.json')\n",
    "    baseline_results_path = Path(data_path, 'baseline_splits.json')\n",
    "    data_baseline = read_data_baseline_new(baseline_results_path)\n",
    "    data_fairboost = read_data_fairboost_new(fairboost_results_path)\n",
    "    return {**data_baseline, **data_fairboost}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def caclculate_accuracy_fairness_h_mean(f1_scores, DI):\n",
    "    \n",
    "    # calculate 1-abs(1-DI)     \n",
    "    normalized_DI = [round(1-abs(1-di_score),6) for di_score in DI]\n",
    "    \n",
    "    harmonic_means = list(map(lambda x, y:statistics.harmonic_mean([x,y]), f1_scores, normalized_DI))\n",
    "    return sum(harmonic_means)/len(harmonic_means)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tuples = []\n",
    "for config, config_info in data.items():\n",
    "    classifier_results = config_info['classifier_value']\n",
    "    h_mean = caclculate_accuracy_fairness_h_mean(classifier_results['f1-score'],  classifier_results['disparate_impact'])\n",
    "    result_tuples.append((config_info['dataset'], config_info['preprocessing'], config_info['classifier'], h_mean))\n",
    "    \n",
    "df = pd.DataFrame(result_tuples, columns=['dataset', 'preprocessing', 'classifier', 'h_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('best_method.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q0HA\\AppData\\Local\\Temp\\ipykernel_12908\\61428036.py:1: FutureWarning: ['classifier'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  df.groupby(['dataset', 'preprocessing',]).agg(['mean']).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>h_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adult</td>\n",
       "      <td>CUSTOM</td>\n",
       "      <td>0.442150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adult</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>0.469858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adult</td>\n",
       "      <td>LFR</td>\n",
       "      <td>0.534773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adult</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.450450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adult</td>\n",
       "      <td>OptimPreproc</td>\n",
       "      <td>0.498712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adult</td>\n",
       "      <td>Reweighing</td>\n",
       "      <td>0.547630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adult</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>compas</td>\n",
       "      <td>CUSTOM</td>\n",
       "      <td>0.739312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compas</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>0.745459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>compas</td>\n",
       "      <td>LFR</td>\n",
       "      <td>0.715122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>compas</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.772440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>compas</td>\n",
       "      <td>OptimPreproc</td>\n",
       "      <td>0.757570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>compas</td>\n",
       "      <td>Reweighing</td>\n",
       "      <td>0.774572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>compas</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.706505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>german</td>\n",
       "      <td>CUSTOM</td>\n",
       "      <td>0.763624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>german</td>\n",
       "      <td>DEFAULT</td>\n",
       "      <td>0.774392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>german</td>\n",
       "      <td>LFR</td>\n",
       "      <td>0.833141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>german</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.805706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>german</td>\n",
       "      <td>OptimPreproc</td>\n",
       "      <td>0.819759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>german</td>\n",
       "      <td>Reweighing</td>\n",
       "      <td>0.848934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>german</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.819451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset preprocessing    h_mean\n",
       "                              mean\n",
       "0    adult        CUSTOM  0.442150\n",
       "1    adult       DEFAULT  0.469858\n",
       "2    adult           LFR  0.534773\n",
       "3    adult          NONE  0.450450\n",
       "4    adult  OptimPreproc  0.498712\n",
       "5    adult    Reweighing  0.547630\n",
       "6    adult      baseline  0.000000\n",
       "7   compas        CUSTOM  0.739312\n",
       "8   compas       DEFAULT  0.745459\n",
       "9   compas           LFR  0.715122\n",
       "10  compas          NONE  0.772440\n",
       "11  compas  OptimPreproc  0.757570\n",
       "12  compas    Reweighing  0.774572\n",
       "13  compas      baseline  0.706505\n",
       "14  german        CUSTOM  0.763624\n",
       "15  german       DEFAULT  0.774392\n",
       "16  german           LFR  0.833141\n",
       "17  german          NONE  0.805706\n",
       "18  german  OptimPreproc  0.819759\n",
       "19  german    Reweighing  0.848934\n",
       "20  german      baseline  0.819451"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1 = df.groupby(['dataset', 'preprocessing',]).agg(['mean']).reset_index()\n",
    "group_1.to_csv('best_method_combined_classifiers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_2 = df.groupby(['preprocessing',]).agg(['mean']).reset_index()\n",
    "group_2.to_csv('best_method_overall.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = df.groupby(['preprocessing',]).agg(['mean']).reset_index()\n",
    "group_2.to_csv('best_method_overall.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}