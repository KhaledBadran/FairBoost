{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# import plotnine as pn\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_baseline_new(path):\n",
    "    \"\"\"\n",
    "    Generates a dict object representing the data in the baseline_splits.json.\n",
    "            Parameters:\n",
    "                    path : path of the file baseline_splits.json\n",
    "            Returns:\n",
    "                    baseline_results (Dict): a preprocessed dict representing the data\n",
    "    \"\"\"\n",
    "    baseline_results = []\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    for dataset_key, dataset_value in results[\"results\"].items():\n",
    "        for preprocessing_key, preprocessing_value in dataset_value.items():\n",
    "            if preprocessing_key == 'baseline':\n",
    "                for classifier_key, performance_metrics in preprocessing_value.items():\n",
    "                    baseline_results.append({'experiment':'baseline', 'bootstrap_type':\"No bootstrap\", 'dataset':dataset_key, 'preprocessing':['No Preprocessing'], 'classifier':classifier_key, 'metrics':performance_metrics})\n",
    "            elif preprocessing_key != \"DisparateImpactRemover\":\n",
    "                for classifier_key, performance_metrics in preprocessing_value[0][\"results\"].items():\n",
    "                    key = dataset_key + \"-\" + preprocessing_key + \"-\" + classifier_key\n",
    "                    baseline_results.append({'experiment':'preprocessing', 'bootstrap_type':\"No bootstrap\", 'dataset':dataset_key, 'preprocessing':[preprocessing_key], 'classifier':classifier_key, 'metrics':performance_metrics})\n",
    "    return baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairboost_run_results(dataset_name:str, raw_run_results:Dict, with_preprocessing:bool) -> List[Dict]:\n",
    "    run_results=[]\n",
    "    \n",
    "    if with_preprocessing:\n",
    "        # this is the fairbosot results (with preprocessing)\n",
    "        experiment = 'fairboost'\n",
    "        \n",
    "        # get the preprocessing methods used in the run (e.g., [LFR, OptimPreproc] or [Reweighing])     \n",
    "        preprocessing_methods = list(raw_run_results['hyperparameters'][\"preprocessing\"].keys())  \n",
    "        \n",
    "        # get the bootstrap method for fairboost (None, Defaul, or Custom)         \n",
    "        bootstrap_method = raw_run_results['hyperparameters'][\"init\"]['bootstrap_type'].lower()\n",
    "        \n",
    "    else:\n",
    "        # since this is a fairboost baseline/normal ensemble we don't have preprocessing\n",
    "        experiment = 'ensemble'\n",
    "        preprocessing_methods = ['No Preprocessing'] \n",
    "        \n",
    "        # get the bootstrap method for the ensemble (None, Defaul, or Custom)         \n",
    "        bootstrap_method = raw_run_results['hyperparameters']['bootstrap_type'].lower()\n",
    "            \n",
    "    # iterate over classifiers to get their performance metrics         \n",
    "    for classifier, performance_metrics in raw_run_results[\"results\"].items():\n",
    "        run_results.append({'experiment':experiment, 'bootstrap_type':bootstrap_method, 'dataset':dataset_name, 'preprocessing':preprocessing_methods, 'classifier':classifier, 'metrics':performance_metrics})\n",
    "    \n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_fairboost_new(path):\n",
    "    \"\"\"\n",
    "    Generates a dict object representing the data in the fairboost_splits.json.\n",
    "            Parameters:\n",
    "                    path : path of the file fairboost_splits.json\n",
    "            Returns:\n",
    "                    dict (Dict): a preprocessed dict representing the data\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    for dataset, dataset_results in results[\"results\"].items():\n",
    "        \n",
    "        # results when ensemble doesn't apply prerpcessing techniques         \n",
    "        ensemble_only_results = dataset_results['baseline']\n",
    "        \n",
    "        for run in ensemble_only_results:\n",
    "            ensemble_run_results = get_fairboost_run_results(dataset_name=dataset, raw_run_results=run, with_preprocessing=False)\n",
    "            all_results.extend(ensemble_run_results)\n",
    "            \n",
    "        \n",
    "        # results for fairboost (ensemble + prerpcessing)         \n",
    "        fairboost_results = dataset_results['fairboost']\n",
    "        \n",
    "        for run in fairboost_results:\n",
    "            fairboost_run_results = get_fairboost_run_results(dataset_name=dataset, raw_run_results=run, with_preprocessing=True)\n",
    "            all_results.extend(fairboost_run_results)\n",
    "            \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def read_data() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Read data from files and return its content as a list of dictionnaries.\n",
    "            Returns:\n",
    "                    data: the data contained in both files\n",
    "    \"\"\"\n",
    "    data_path = Path(\"raw_data\")\n",
    "    fairboost_results_path = Path(data_path, 'fairboost_splits.json')\n",
    "    baseline_results_path = Path(data_path, 'baseline_splits.json')\n",
    "    data_baseline = read_data_baseline_new(baseline_results_path)\n",
    "    data_fairboost = read_data_fairboost_new(fairboost_results_path)\n",
    "    return data_baseline + data_fairboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>bootstrap_type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>classifier</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>No bootstrap</td>\n",
       "      <td>german</td>\n",
       "      <td>[No Preprocessing]</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'accuracy': [0.7333333333333333, 0.68, 0.6666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>No bootstrap</td>\n",
       "      <td>german</td>\n",
       "      <td>[No Preprocessing]</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'accuracy': [0.69, 0.66, 0.69, 0.683333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preprocessing</td>\n",
       "      <td>No bootstrap</td>\n",
       "      <td>german</td>\n",
       "      <td>[OptimPreproc]</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'accuracy': [0.7066666666666667, 0.6766666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preprocessing</td>\n",
       "      <td>No bootstrap</td>\n",
       "      <td>german</td>\n",
       "      <td>[OptimPreproc]</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'accuracy': [0.7133333333333334, 0.66, 0.6866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>preprocessing</td>\n",
       "      <td>No bootstrap</td>\n",
       "      <td>german</td>\n",
       "      <td>[LFR]</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'accuracy': [0.7133333333333334, 0.6833333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>fairboost</td>\n",
       "      <td>custom</td>\n",
       "      <td>compas</td>\n",
       "      <td>[OptimPreproc, Reweighing]</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'accuracy': [0.6287878787878788, 0.6470959595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>fairboost</td>\n",
       "      <td>custom</td>\n",
       "      <td>compas</td>\n",
       "      <td>[LFR, Reweighing]</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'accuracy': [0.6515151515151515, 0.6597222222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>fairboost</td>\n",
       "      <td>custom</td>\n",
       "      <td>compas</td>\n",
       "      <td>[LFR, Reweighing]</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'accuracy': [0.6534090909090909, 0.6559343434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>fairboost</td>\n",
       "      <td>custom</td>\n",
       "      <td>compas</td>\n",
       "      <td>[LFR, OptimPreproc, Reweighing]</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'accuracy': [0.6540404040404041, 0.6679292929...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>fairboost</td>\n",
       "      <td>custom</td>\n",
       "      <td>compas</td>\n",
       "      <td>[LFR, OptimPreproc, Reweighing]</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'accuracy': [0.6452020202020202, 0.6483585858...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        experiment bootstrap_type dataset                    preprocessing  \\\n",
       "0         baseline   No bootstrap  german               [No Preprocessing]   \n",
       "1         baseline   No bootstrap  german               [No Preprocessing]   \n",
       "2    preprocessing   No bootstrap  german                   [OptimPreproc]   \n",
       "3    preprocessing   No bootstrap  german                   [OptimPreproc]   \n",
       "4    preprocessing   No bootstrap  german                            [LFR]   \n",
       "..             ...            ...     ...                              ...   \n",
       "145      fairboost         custom  compas       [OptimPreproc, Reweighing]   \n",
       "146      fairboost         custom  compas                [LFR, Reweighing]   \n",
       "147      fairboost         custom  compas                [LFR, Reweighing]   \n",
       "148      fairboost         custom  compas  [LFR, OptimPreproc, Reweighing]   \n",
       "149      fairboost         custom  compas  [LFR, OptimPreproc, Reweighing]   \n",
       "\n",
       "              classifier                                            metrics  \n",
       "0    Logistic Regression  {'accuracy': [0.7333333333333333, 0.68, 0.6666...  \n",
       "1          Random Forest  {'accuracy': [0.69, 0.66, 0.69, 0.683333333333...  \n",
       "2    Logistic Regression  {'accuracy': [0.7066666666666667, 0.6766666666...  \n",
       "3          Random Forest  {'accuracy': [0.7133333333333334, 0.66, 0.6866...  \n",
       "4    Logistic Regression  {'accuracy': [0.7133333333333334, 0.6833333333...  \n",
       "..                   ...                                                ...  \n",
       "145        Random Forest  {'accuracy': [0.6287878787878788, 0.6470959595...  \n",
       "146  Logistic Regression  {'accuracy': [0.6515151515151515, 0.6597222222...  \n",
       "147        Random Forest  {'accuracy': [0.6534090909090909, 0.6559343434...  \n",
       "148  Logistic Regression  {'accuracy': [0.6540404040404041, 0.6679292929...  \n",
       "149        Random Forest  {'accuracy': [0.6452020202020202, 0.6483585858...  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_data()\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def caclculate_accuracy_fairness_h_mean(f1_scores, normalized_di_scores):\n",
    "    \n",
    "    # TODO: find method for normalization that doesn't return negative values     \n",
    "#     normalized_DI = [(1-di_score) if di_score <=1 else (1-di_score**-1) for di_score in DI]\n",
    "    \n",
    "    try:\n",
    "        harmonic_means = list(map(lambda x, y:statistics.harmonic_mean([x,y]), f1_scores, normalized_di_scores))\n",
    "    except Exception as e:\n",
    "        print('faced error in harmonic mean calculation')\n",
    "        print(f'f1_scores: {f1_scores}')\n",
    "        print(f'DI: {DI}')\n",
    "        print(f'normalized_DI: {normalized_DI}')\n",
    "        print(e)\n",
    "    return sum(harmonic_means)/len(harmonic_means)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mean_scores = []\n",
    "f1_scores = []\n",
    "normalized_di_scores = []\n",
    "for performance_metric in df.loc[:,\"metrics\"]:\n",
    "    \n",
    "    f1_score = performance_metric[\"f1-score\"]\n",
    "    f1_scores.append(f1_score)\n",
    "    \n",
    "    di_score = performance_metric[\"disparate_impact\"]\n",
    "    normalized_di_score = [score if score <=1 else (score**-1) for score in di_score]\n",
    "    normalized_di_scores.append(normalized_di_score)\n",
    "    \n",
    "    h_mean_score = caclculate_accuracy_fairness_h_mean(f1_score, normalized_di_score)\n",
    "    h_mean_scores.append(h_mean_score)\n",
    "    \n",
    "\n",
    "# df['f1_scores'] = f1_scores\n",
    "# df['normalized_di_scores'] = normalized_di_scores\n",
    "df['h_mean'] = h_mean_scores\n",
    "df.drop(['metrics'], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_join(l):\n",
    "    try:\n",
    "        return ','.join(map(str, l))\n",
    "    except TypeError:\n",
    "        return 'No Preprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessing\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [try_join(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessing\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# df2.drop(['preprocessing'], axis = 1, inplace=True)\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mdf2\u001B[49m\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m60\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "df['preprocessing'] = [try_join(l) for l in df['preprocessing']]\n",
    "# df2.drop(['preprocessing'], axis = 1, inplace=True)\n",
    "df2.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df2.groupby(['experiment','bootstrap_type','dataset', 'preprocessing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3 = groups['h_mean'].agg([np.mean]).reset_index()\n",
    "df3.sort_values(by=['mean'], ascending=False)\n",
    "df3['setting'] = df['experiment'] + ', ' + df['bootstrap_type'] + ', ' + df['preprocessing']\n",
    "df3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tuples = []\n",
    "# for config, config_info in data.items():\n",
    "#     classifier_results = config_info['classifier_value']\n",
    "#     h_mean = caclculate_accuracy_fairness_h_mean(classifier_results['f1-score'],  classifier_results['disparate_impact'])\n",
    "#     result_tuples.append((config_info['dataset'], config_info['preprocessing'], config_info['classifier'], h_mean))\n",
    "    \n",
    "# df = pd.DataFrame(result_tuples, columns=['dataset', 'preprocessing', 'classifier', 'h_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tuples = []\n",
    "# for config, config_info in data.items():\n",
    "#     classifier_results = config_info['classifier_value']\n",
    "#     h_mean = caclculate_accuracy_fairness_h_mean(classifier_results['f1-score'],  classifier_results['disparate_impact'])\n",
    "#     result_tuples.append((config_info['dataset'], config_info['preprocessing'], config_info['classifier'], h_mean))\n",
    "    \n",
    "# df = pd.DataFrame(result_tuples, columns=['dataset', 'preprocessing', 'classifier', 'h_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('best_method.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_1 = df.groupby(['experiment','bootstrap_type','dataset','preprocessing',]).agg(['mean']).reset_index()\n",
    "# group_1\n",
    "# group_1.to_csv('best_method_combined_classifiers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_2 = df.groupby(['preprocessing',]).agg(['mean']).reset_index()\n",
    "group_2.to_csv('best_method_overall.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = df.groupby(['preprocessing',]).agg(['mean']).reset_index()\n",
    "group_2.to_csv('best_method_overall.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}