{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8756fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://github.com/Trusted-AI/AIF360/blob/master/examples/tutorial_medical_expenditure.ipynb\n",
    "# https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_meta_classifier.ipynb\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.datasets import GermanDataset\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feae374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [AdultDataset(), CompasDataset(), GermanDataset(protected_attribute_names=['sex'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39444d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GermanDataset(protected_attribute_names=['sex'])\n",
    "dataset_name = 'german'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b254120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_val, dataset_orig_test = dataset.split([0.5, 0.8], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24237efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find priviliged and unprivliged groups\n",
    "# TODO: make this a constant\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4742ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to describe the dataset\n",
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(test.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb986005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 58)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Validation Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 58)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 58)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'credit_amount', 'investment_as_income_percentage', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'sex', 'status=A11', 'status=A12', 'status=A13', 'status=A14', 'credit_history=A30', 'credit_history=A31', 'credit_history=A32', 'credit_history=A33', 'credit_history=A34', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=A61', 'savings=A62', 'savings=A63', 'savings=A64', 'savings=A65', 'employment=A71', 'employment=A72', 'employment=A73', 'employment=A74', 'employment=A75', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'housing=A151', 'housing=A152', 'housing=A153', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"
     ]
    }
   ],
   "source": [
    "describe(dataset_orig_train, dataset_orig_val, dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9829eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LR model\n",
    "pipeline = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='liblinear', random_state=1))\n",
    "# fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n",
    "\n",
    "model = pipeline.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f393ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to find the best threshold using the validation set\n",
    "from collections import defaultdict\n",
    "\n",
    "def test(dataset, model, thresh_arr):\n",
    "    try:\n",
    "        # sklearn classifier\n",
    "        y_val_pred_prob = model.predict_proba(dataset.features)\n",
    "        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n",
    "    except AttributeError:\n",
    "        # aif360 inprocessing algorithm\n",
    "        y_val_pred_prob = model.predict(dataset).scores\n",
    "        pos_ind = 0\n",
    "    \n",
    "    metric_arrs = defaultdict(list)\n",
    "    for thresh in thresh_arr:\n",
    "        y_val_pred = (y_val_pred_prob[:, pos_ind] > thresh).astype(np.float64)\n",
    "\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = y_val_pred\n",
    "        metric = ClassificationMetric(\n",
    "                dataset, dataset_pred,\n",
    "                unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_arrs['bal_acc'].append((metric.true_positive_rate()\n",
    "                                     + metric.true_negative_rate()) / 2)\n",
    "        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n",
    "        metric_arrs['disp_imp'].append(metric.disparate_impact())\n",
    "        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n",
    "        metric_arrs['eq_opp_diff'].append(metric.equal_opportunity_difference())\n",
    "        metric_arrs['theil_ind'].append(metric.theil_index())\n",
    "    \n",
    "    return metric_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc449b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to describe metrics\n",
    "\n",
    "def describe_metrics(metrics, thresh_arr):\n",
    "    best_ind = np.argmax(metrics['bal_acc'])\n",
    "    print(\"Threshold corresponding to Best balanced accuracy: {:6.4f}\".format(thresh_arr[best_ind]))\n",
    "    print(\"Best balanced accuracy: {:6.4f}\".format(metrics['bal_acc'][best_ind]))\n",
    "#     disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]\n",
    "    disp_imp_at_best_ind = 1 - min(metrics['disp_imp'][best_ind], 1/metrics['disp_imp'][best_ind])\n",
    "    print(\"Corresponding 1-min(DI, 1/DI) value: {:6.4f}\".format(disp_imp_at_best_ind))\n",
    "    print(\"Corresponding average odds difference value: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n",
    "    print(\"Corresponding statistical parity difference value: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n",
    "    print(\"Corresponding equal opportunity difference value: {:6.4f}\".format(metrics['eq_opp_diff'][best_ind]))\n",
    "    print(\"Corresponding Theil index value: {:6.4f}\".format(metrics['theil_ind'][best_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8a026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_val,\n",
    "                   model=model,\n",
    "                   thresh_arr=thresh_arr)\n",
    "model_orig_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84268bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.4976\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.0049\n",
      "Corresponding average odds difference value: 0.0034\n",
      "Corresponding statistical parity difference value: 0.0049\n",
      "Corresponding equal opportunity difference value: 0.0068\n",
      "Corresponding Theil index value: 0.0614\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(val_metrics, thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model_orig_metrics = test(dataset=dataset_orig_test,\n",
    "                       model=model,\n",
    "                       thresh_arr=[thresh_arr[model_orig_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df8a276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.5000\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.0000\n",
      "Corresponding average odds difference value: 0.0000\n",
      "Corresponding statistical parity difference value: 0.0000\n",
      "Corresponding equal opportunity difference value: 0.0000\n",
      "Corresponding Theil index value: 0.0582\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(model_orig_metrics, [thresh_arr[model_orig_best_ind]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29512df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reweighing\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c6896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transf = pipeline.fit(dataset_transf_train.features, dataset_transf_train.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56556fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_arr = np.linspace(0.01, 0.5, 50)\n",
    "val_metrics = test(dataset=dataset_orig_val,\n",
    "                   model=model_transf,\n",
    "                   thresh_arr=thresh_arr)\n",
    "model_transf_best_ind = np.argmax(val_metrics['bal_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a45dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model_transf_metrics = test(dataset=dataset_orig_test,\n",
    "                         model=model_transf,\n",
    "                         thresh_arr=[thresh_arr[model_transf_best_ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a934e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold corresponding to Best balanced accuracy: 0.0100\n",
      "Best balanced accuracy: 0.5000\n",
      "Corresponding 1-min(DI, 1/DI) value: 0.0000\n",
      "Corresponding average odds difference value: 0.0000\n",
      "Corresponding statistical parity difference value: 0.0000\n",
      "Corresponding equal opportunity difference value: 0.0000\n",
      "Corresponding Theil index value: 0.0582\n"
     ]
    }
   ],
   "source": [
    "describe_metrics(model_transf_metrics, [thresh_arr[model_transf_best_ind]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7e34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
